{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avito Demand Prediction Challenge\n",
    "\n",
    "Контест Avito по предсказанию успешности объявления, основываясь на его тексте и изображениях.\n",
    "\n",
    "В рамках курса \"Автоматическая обработка естественного языка\" контест решали: \n",
    "* Дима Татаринов, БКЛ152,\n",
    "* Даша Максимова, БКЛ151.\n",
    "\n",
    "\n",
    "P.S.\n",
    "\n",
    "К сожалению, при написании отчета, код обнулился и его пришлось запускать заново. Из-за чего большинство выводов нет."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Данные\n",
    "\n",
    "```\n",
    "!mkdir data\n",
    "!gsutil cp -r gs://python-ml-hse/avito/data/data .\n",
    "```\n",
    "или\n",
    "\n",
    "```\n",
    "!gsutil cp -r gs://python-ml-hse/avito/data/data/train.csv ./data/train.csv\n",
    "!gsutil cp -r gs://python-ml-hse/avito/data/data/test.csv ./data/test.csv\n",
    "```\n",
    "\n",
    "### 1.1. Загрузка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/train.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В датасете имеются такие колонки-фичи:\n",
    "\n",
    "* `item_id` — Ad id.\n",
    "* `user_id` — User id.\n",
    "* `region` — Ad region.\n",
    "* `city` — Ad city.\n",
    "* `parent_category_name` — Top level ad category as classified by Avito's ad model.\n",
    "* `category_name` — Fine grain ad category as classified by Avito's ad model.\n",
    "* `param_1` — Optional parameter from Avito's ad model.\n",
    "* `param_2` — Optional parameter from Avito's ad model.\n",
    "* `param_3` — Optional parameter from Avito's ad model.\n",
    "* `title` — Ad title.\n",
    "* `description` — Ad description.\n",
    "* `price` — Ad price.\n",
    "* `item_seq_number` — Ad sequential number for user.\n",
    "* `activation_date`— Date ad was placed.\n",
    "* `user_type` — User type.\n",
    "* `image` — Id code of image. Ties to a jpg file in train_jpg. Not every ad has an image.\n",
    "* `image_top_1` — Avito's classification code for the image.\n",
    "* `deal_probability` — The target variable. This is the likelihood that an ad actually sold something. It's not possible to verify every transaction with certainty, so this column's value can be any float from zero to one.\n",
    "\n",
    "\n",
    "Мы уберём следующие из них:\n",
    "* `item_id`, `user_id` — это бесполезная для нас информация,\n",
    "* `city` — кажется слишком мелким делением,\n",
    "* `title` — у нас есть более содержательные тексты,\n",
    "* `param_1`, `param_2`, `param_3` — опциональные и не всегда присутствующие параметры,\n",
    "* `activation_date`, `item_seq_number` — тоже выглядит бесполезной,\n",
    "* `image`, `image_top_1` — мы сконцентрируемся на текстах и категориальных переменных, а не на изображениях."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols_to_drop = [\"item_id\", \"user_id\", \"city\", \"param_1\", \"param_2\", \"param_3\", \"title\",\n",
    "    \"activation_date\", \"item_seq_number\", \"image\", \"image_top_1\"]\n",
    "data = data.drop(labels=cols_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Категориальные признаки закодируем (преобразуем в числовой вид):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_category = LabelEncoder()\n",
    "parent_category.fit(data[\"parent_category_name\"])\n",
    "data[\"parent_category_name\"] = parent_category.transform(data[\"parent_category_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "category = LabelEncoder()\n",
    "category.fit(data[\"category_name\"])\n",
    "data[\"category_name\"] = category.transform(data[\"category_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_type = LabelEncoder()\n",
    "user_type.fit(data[\"user_type\"])\n",
    "data[\"user_type\"] = user_type.transform(data[\"user_type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "region = LabelEncoder()\n",
    "region.fit(data[\"region\"])\n",
    "data[\"region\"] = region.transform(data[\"region\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так датасет выглядит сейчас:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Предобработка: тексты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Экспериментальным путём выяснено, что некоторые объявления пустые. Чтобы об них ничего не ломалось, запустим `fillna()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data[\"description\"].fillna(\"\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тексты сначала приведём к нижнему регистру, а затем посчитаем количество токенов и приведём всё к леммам. По пути уберём стоп-слова. Это можно сделать так (Данный процесс занимает очень много времени, поэтому при выгрузки данных препроцессинг ограничен):\n",
    "\n",
    "```python\n",
    "from nltk import word_tokenize\n",
    "from pymystem3 import Mystem\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "mystem = Mystem()\n",
    "\n",
    "\n",
    "def count_words(text):\n",
    "    try:\n",
    "        len_words = len(word_tokenize(text))\n",
    "    except:\n",
    "        len_words = 0\n",
    "    return len_words\n",
    "\n",
    "def do_lemmas(text):\n",
    "    try:\n",
    "        stops = stopwords.words(\"russian\")\n",
    "        lemmas = [lemma for lemma in mystem.lemmatize(text) if lemma not in stops]\n",
    "        return lemmas\n",
    "    except:\n",
    "        return \"\"\n",
    "        \n",
    "data_new[\"word_count\"] = data_new[\"description\"].apply(count_words)\n",
    "data_new[\"lemmas\"] = data_new[\"description\"].apply(do_lemmas)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем модуль casual_tokenize из библиотеки nltk для токенизации текста.\n",
    "\n",
    "Чистка текста от знаков препинания с помощью втроенной функции Python .isalpha()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import casual_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    tokens = casual_tokenize(str(text))\n",
    "    clean_stuff = [word.lower() for word in tokens if word.isalpha()]\n",
    "    line = \" \".join(clean_stuff)\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data[\"description\"] = data[\"description\"].apply(tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. TF-IDF текстов + отделение фич"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала выделим целевую переменную и все категориальные и количественные фичи:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat_num_cols = [\"region\", \"parent_category_name\", \"category_name\", \"price\", \"user_type\"]\n",
    "X_cat_num = data[cat_num_cols].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = data[\"deal_probability\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для стабильного решения проблемы с текстами, с помощью TF-IDF + SVM необходимо использовать PCA (метод главных компонентов, про который подробнее можно ознакоимться по ссылке: https://habr.com/post/304214/), или же ограничить размерность векторов получаемых методом TF-IDF для избавления от данных, которые влиют на результат не значительно. \n",
    "\n",
    "Применим TF-IDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents=\"unicode\",\n",
    "    analyzer=\"word\",\n",
    "    token_pattern=r\"\\w{1,}\",\n",
    "    stop_words=stopwords.words(\"russian\"),\n",
    "    max_features=10000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tfidf.fit(data[\"description\"].values)\n",
    "X_texts = tfidf.transform(data[\"description\"].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Объединение фич"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На данном этапе неоходимо соединить все полученные нами катег. признаки с результатом работы TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = hstack((X_texts, X_cat_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраняем на всякий случай:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.mkdir(\"./models\")\n",
    "except:\n",
    "    pass\n",
    "joblib.dump(X, \"./models/X.pkl\")\n",
    "joblib.dump(y, \"./models/y.pkl\")\n",
    "joblib.dump(tfidf, \"./models/tfidf.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Всякие разные алгоритмы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0. Подготовка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала **разобьём выборку** на обучающую и тестовую:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Создадим функцию ошибки** RMSE (root of mean squared error, корень из средней квадратичной ошибки), чтобы оценивать по ней:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rmse_func(y_calc, y_test):\n",
    "    rms = sqrt(mean_squared_error(y_actual, y_predicted))\n",
    "    return rms\n",
    "\n",
    "rmse = make_scorer(rmse_func, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. SVM\n",
    "\n",
    "Классическое state of the art решение для задач на текстах — SVM на RBF-ядре."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env JOBLIB_TEMP_FOLDER=/tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_svr = {\"C\": np.arange(0.1, 100.1, 0.1)}\n",
    "svr = GridSearchCV(\n",
    "    SVR(),\n",
    "    param_grid=params_svc,\n",
    "    scoring=rmse,\n",
    "    cv=5,\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "svr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"SVR results:\\n\\t- best params: {}\\n\\t- best score: {}\".format(svc.best_params_, svc.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = svr.predict(y)\n",
    "ids = data['item_id']\n",
    "ids['deal_probability'] = result\n",
    "ids.to_csv(\"submit2.csv\",index=True,header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
